{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAYkAM9tIWbNqKxUWMIPha"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvcVyqsalxsD"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b15yQ0ACl3yY"
      },
      "source": [
        "RE = \"Wind_Wallonie_Elia\" # Solar_PBE Wind_Wallonie_Elia\n",
        "address = \"https://raw.githubusercontent.com/Jaeik-Jeong/DeepBid/main/\"\n",
        "\n",
        "data_train_csv1 = pd.read_csv(address+RE+'_16.csv', index_col=0)\n",
        "data_train_csv2 = pd.read_csv(address+RE+'_17.csv', index_col=0)\n",
        "data_train_csv  = pd.concat([data_train_csv1, data_train_csv2])\n",
        "data_val_csv    = pd.read_csv(address+RE+'_18.csv', index_col=0)\n",
        "data_test_csv   = pd.read_csv(address+RE+'_19.csv', index_col=0)\n",
        "\n",
        "data_price = pd.read_csv(address+'Elia_Imbalance_Price_16_19.csv', index_col=0)\n",
        "data_train_csv['Price(€)'] = data_price['Positive imbalance price'][:len(data_train_csv)]\n",
        "data_val_csv['Price(€)']   = data_price['Positive imbalance price'][len(data_train_csv):len(data_train_csv)+len(data_val_csv)]\n",
        "data_test_csv['Price(€)']  = data_price['Positive imbalance price'][len(data_train_csv)+len(data_val_csv):]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB4phR_Vl4Xg"
      },
      "source": [
        "# Data Preprocessing\n",
        " \n",
        "Battery_Size = 0.15 #p.u.\n",
        "unit         = 1 #unit: 15 minute\n",
        " \n",
        "RE_Capacity1 = max(data_train_csv['Power(MW)'])\n",
        "RE_Capacity2 = max(data_val_csv['Power(MW)'])\n",
        "RE_Capacity3 = max(data_test_csv['Power(MW)'])\n",
        "max_price = max(data_price['Marginal incremental price'])\n",
        " \n",
        "size_train0 = int(len(data_train_csv)/unit)\n",
        "size_val0   = int(len(data_val_csv)/unit)\n",
        "size_test0  = int(len(data_test_csv)/unit)\n",
        " \n",
        "data_train0 = []; data_train = []; price_train0 = []; price_train = [];\n",
        "for i in range(size_train0):\n",
        "    data_train0  += [round(pd.Series.mean(data_train_csv['Power(MW)'][i*unit:(i+1)*unit])/RE_Capacity1, 3)]\n",
        "    price_train0 += [round(pd.Series.mean(data_train_csv['Price(€)'][i*unit:(i+1)*unit])/max_price, 3)]\n",
        "    if data_train0[i] > 0: data_train += [data_train0[i]]; price_train += [price_train0[i]]\n",
        " \n",
        "data_val0 = []; data_val = []; price_val0 = []; price_val = []\n",
        "for i in range(size_val0):\n",
        "    data_val0  += [round(pd.Series.mean(data_val_csv['Power(MW)'][i*unit:(i+1)*unit])/RE_Capacity2, 3)]\n",
        "    price_val0 += [round(pd.Series.mean(data_val_csv['Price(€)'][i*unit:(i+1)*unit])/max_price, 3)]\n",
        "    if data_val0[i] > 0: data_val += [data_val0[i]]; price_val += [price_val0[i]]\n",
        " \n",
        "data_test0 = []; data_test = []; price_test0 = []; price_test = []\n",
        "for i in range(size_test0):\n",
        "    data_test0  += [round(pd.Series.mean(data_test_csv['Power(MW)'][i*unit:(i+1)*unit])/RE_Capacity3, 3)]\n",
        "    price_test0 += [round(pd.Series.mean(data_test_csv['Price(€)'][i*unit:(i+1)*unit])/max_price, 3)]\n",
        "    if data_test0[i] > 0: data_test += [data_test0[i]]; price_test += [price_test0[i]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rr6hJBuvl5XY"
      },
      "source": [
        "# PPO Agent (Partailly Observable State, Continuous Action Space)\n",
        "# Assumption 1: Standard deviation is fixed\n",
        "# Assumption 2: History is composed of observations only\n",
        " \n",
        "n_layers         = 2\n",
        "in_size          = 2\n",
        "hidden_size      = 64\n",
        "out_size         = 1\n",
        "T_horizon        = 128\n",
        "learning_rate    = 0.001\n",
        "K_epoch          = 3\n",
        "gamma            = 0.99\n",
        "lmbda            = 0.95\n",
        "eps_clip         = 0.01\n",
        "C_value          = 1\n",
        "var              = 0.1**2\n",
        " \n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.fc_s  = nn.Linear(in_size, hidden_size)\n",
        "        self.rnn   = nn.LSTM(hidden_size, hidden_size, n_layers, batch_first=True)\n",
        "        self.fc_pi = nn.Linear(hidden_size, out_size)\n",
        "        self.fc_v  = nn.Linear(hidden_size, 1)\n",
        " \n",
        "    def pi(self, x, hidden):\n",
        "        x = F.relu(self.fc_s(x))\n",
        "        x = x.view(1, -1, hidden_size)\n",
        "        x, hidden = self.rnn(x, hidden)\n",
        "        pi = self.fc_pi(x)\n",
        "        pi = pi.view(-1, out_size)\n",
        "        return pi, hidden\n",
        "    \n",
        "    def v(self, x, hidden):\n",
        "        x = F.relu(self.fc_s(x))\n",
        "        x = x.view(1, -1, hidden_size)\n",
        "        x, hidden = self.rnn(x, hidden)\n",
        "        v = self.fc_v(x)\n",
        "        v = v.view(-1, 1)\n",
        "        return v\n",
        " \n",
        "def train_net(model, batch, optimizer):\n",
        "    o, H, a, r, o_prime, H_prime, done = [], [], [], [], [], [], []\n",
        "    for transition in batch[0]:\n",
        "        o.append(transition[0])\n",
        "        a.append(transition[1])\n",
        "        r.append([transition[2]])\n",
        "        o_prime.append(transition[3])\n",
        "        done.append([0]) if transition[4] else done.append([1])\n",
        "    for transition in batch[1]:\n",
        "        H.append(transition[0])\n",
        "        H_prime.append(transition[1])\n",
        "        \n",
        "    o         = torch.tensor(o,dtype=torch.float)\n",
        "    H         = (H[0][0].detach(), H[0][1].detach())\n",
        "    a         = torch.tensor(a,dtype=torch.float)\n",
        "    r         = torch.tensor(r,dtype=torch.float)\n",
        "    o_prime   = torch.tensor(o_prime,dtype=torch.float)\n",
        "    H_prime   = (H_prime[0][0].detach(), H_prime[0][1].detach())\n",
        "    done      = torch.tensor(done)\n",
        " \n",
        "    pdf_old = torch.distributions.MultivariateNormal(model.pi(o, H)[0], var*torch.eye(out_size))\n",
        "    prob_old = torch.exp(pdf_old.log_prob(a)).view(len(a),1)\n",
        "    prob_old = prob_old.detach()\n",
        " \n",
        "    v_target = r + gamma * model.v(o_prime, H_prime) * done\n",
        "    td = r + gamma * model.v(o_prime, H_prime) * done - model.v(o, H)\n",
        "    td = td.detach().numpy()\n",
        "    advantage = []\n",
        "    A = 0.0\n",
        "    for delta in td[::-1].flatten():\n",
        "        A = delta + gamma*lmbda*A\n",
        "        advantage.append([A])\n",
        "    advantage.reverse()\n",
        "    advantage = torch.tensor(advantage, dtype=torch.float)\n",
        "    \n",
        "    for i in range(K_epoch):\n",
        "        pdf = torch.distributions.MultivariateNormal(model.pi(o, H)[0], var*torch.eye(out_size))\n",
        "        prob = torch.exp(pdf.log_prob(a)).view(len(a),1)\n",
        "        ratio = torch.exp(torch.log(prob) - torch.log(prob_old))  # a/b == exp(log(a)-log(b))\n",
        " \n",
        "        loss_actor = torch.min(ratio * advantage, torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage)\n",
        "        loss_critic = F.mse_loss(model.v(o, H), v_target.detach())\n",
        "        loss = -(loss_actor - C_value*loss_critic)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.mean().backward(retain_graph=True)\n",
        "        optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LdaWXmdl6GY"
      },
      "source": [
        "# Environment\n",
        " \n",
        "E_max   = Battery_Size\n",
        "P_max   = E_max\n",
        "tdelta  = unit/4\n",
        "soc_min = 0.1\n",
        "soc_max = 0.9\n",
        "a0 = -1.031; a1 = 35; a2 = 3.685; a3 = 0.2156; a4 = 0.1178; a5 = 0.3201\n",
        "b0 = 0.1463; b1 = 30.27; b2 = 0.1037; b3 = 0.0584; b4 = 0.1747; b5 = 0.1288\n",
        "c0 = 0.1063; c1 = 62.49; c2 = 0.0437; d0 = 0.0712; d1 = 61.4; d2 = 0.0288\n",
        "N = 130*215*E_max/0.1\n",
        "beta = 10/max_price\n",
        " \n",
        "class Env():\n",
        "    def __init__(self, data, price):\n",
        "        self.data = data\n",
        "        self.price = price\n",
        "        self.state = []\n",
        " \n",
        "    def reset(self):\n",
        "        gen = self.data[0]\n",
        "        E = E_max/2\n",
        "        state = [[gen, E]]\n",
        "        self.state = state\n",
        "        return state\n",
        " \n",
        "    def step(self, action):\n",
        "        gen = self.data[len(self.state)]\n",
        "        bid = action[0]\n",
        "        imb = self.price[len(self.state)]\n",
        "\n",
        "        E = self.state[-1][-1]\n",
        "        soc = E/E_max\n",
        "        Voc = a0*np.exp(-a1*soc) + a2 + a3*soc - a4*soc**2 + a5*soc**3\n",
        "        Rs  = b0*np.exp(-b1*soc) + b2 + b3*soc - b4*soc**2 + b5*soc**3\n",
        "        Rts = c0*np.exp(-c1*soc) + c2\n",
        "        Rtl = d0*np.exp(-d1*soc) + d2\n",
        "        R   = Rs + Rts + Rtl\n",
        "\n",
        "        I_cmax = 1000000*(E_max*soc_max - E)/N/(Voc*tdelta)\n",
        "        I_dmax = 1000000*(E - E_max*soc_min)/N/(Voc*tdelta)\n",
        "        p_cmax = N*(Voc*I_cmax + I_cmax**2*R)\n",
        "        p_dmax = N*(Voc*I_dmax - I_dmax**2*R)\n",
        "\n",
        "        P_cmax = p_cmax/1000000; P_dmax = p_dmax/1000000\n",
        "        P_c = min(max(gen-bid, 0), P_max, P_cmax)\n",
        "        P_d = min(max(bid-gen, 0), P_max, P_dmax)\n",
        "        p_c = 1000000*P_c/N; p_d = 1000000*P_d/N\n",
        "\n",
        "        I_c = -(Voc - np.sqrt(Voc**2 + 4*R*p_c))/(2*R)\n",
        "        I_d = (Voc - np.sqrt(Voc**2 - 4*R*p_d))/(2*R)\n",
        "        if not np.isclose(p_c, 0):\n",
        "            eff_c = (Voc*I_c)/p_c; eff_d = 1\n",
        "            E_prime = E + eff_c*P_c*tdelta\n",
        "            disp = gen - P_c\n",
        "        elif not np.isclose(p_d, 0):\n",
        "            eff_d = p_d/(Voc*I_d); eff_c = 1\n",
        "            E_prime = E - (1/eff_d)*P_d*tdelta\n",
        "            disp = gen + P_d\n",
        "        else:\n",
        "            eff_c = 1; eff_d = 1\n",
        "            E_prime = E\n",
        "            disp = gen\n",
        "\n",
        "        error = bid - disp\n",
        "        error_function = abs(error) + beta*P_c + beta*P_d\n",
        "        revenue = (imb*disp - imb*abs(bid-disp) - beta*(P_c+P_d))*tdelta\n",
        " \n",
        "        next_state = state + [[gen, E_prime]]\n",
        "        reward = -error_function\n",
        "        done = False\n",
        "        info = [gen, bid, disp, revenue]\n",
        " \n",
        "        self.state = next_state\n",
        "        return next_state, reward, done, info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zuc-HkHnl605"
      },
      "source": [
        "# PPO Training\n",
        "\n",
        "total_episode = 500\n",
        "max_iteration = int(len(data_train)/T_horizon)\n",
        "print_interval = 1\n",
        " \n",
        "model = LSTM()\n",
        "env_train = Env(data_train, price_train)\n",
        "env_val   = Env(data_val, price_val)\n",
        "env_test  = Env(data_test, price_test)\n",
        "bid_train, bid_val, bid_test = [], [], [] # Bidding Value\n",
        "mae_train, mae_val, mae_test = [], [], [] # Mean Absolute Error\n",
        "mbe_train, mbe_val, mbe_test = [], [], [] # Mean Bidding Error\n",
        "rev_train, rev_val, rev_test = [], [], [] # Revenue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOhpOvKRl7mo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "outputId": "9621154c-185c-428c-c2b9-4378500f242a"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "for n_epi in range(total_episode):\n",
        "    bid_train += [[]]; bid_val += [[]]; bid_test += [[]]\n",
        "    mae_train += [[]]; mae_val += [[]]; mae_test += [[]]\n",
        "    mbe_train += [[]]; mbe_val += [[]]; mbe_test += [[]]\n",
        "    rev_train += [[]]; rev_val += [[]]; rev_test += [[]]\n",
        " \n",
        "    state = env_train.reset()\n",
        "    history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
        "    for i in range(max_iteration):\n",
        "        batch = [[],[]]\n",
        "        for t in range(T_horizon):\n",
        "            pi_out, next_history = model.pi(torch.tensor(state[-1], dtype=torch.float), history)\n",
        "            action = np.random.multivariate_normal(pi_out.detach().numpy()[0], var*np.identity(out_size), 1)[0].tolist()\n",
        "            next_state, reward, done, info = env_train.step(action)\n",
        " \n",
        "            batch[0].append((state[-1], action, reward, next_state[-1], done))\n",
        "            batch[1].append((history, next_history))\n",
        "            state = next_state[:]\n",
        "            history = next_history\n",
        " \n",
        "            gen = info[0]; bid = info[1]; disp = info[2]; revenue = info[3]\n",
        "            bid_train[n_epi] += [bid]\n",
        "            mae_train[n_epi] += [abs(gen - bid)]\n",
        "            mbe_train[n_epi] += [abs(disp - bid)]\n",
        "            rev_train[n_epi] += [revenue]\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        if n_epi != 0:\n",
        "            train_net(model, batch, optimizer)\n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    state = env_val.reset()\n",
        "    history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
        "    for k in range(len(env_val.data)-1):\n",
        "        pi_out, next_history = model.pi(torch.tensor(state[-1], dtype=torch.float), history)\n",
        "        action = pi_out[0].tolist()\n",
        "        next_state, reward, done, info = env_val.step(action)\n",
        " \n",
        "        state = next_state[:]\n",
        "        history = next_history\n",
        "        \n",
        "        gen = info[0]; bid = info[1]; disp = info[2]; revenue = info[3]\n",
        "        bid_val[n_epi] += [bid]\n",
        "        mae_val[n_epi] += [abs(gen - bid)]\n",
        "        mbe_val[n_epi] += [abs(disp - bid)]\n",
        "        rev_val[n_epi] += [revenue]\n",
        "    \n",
        "    state = env_test.reset()\n",
        "    history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
        "    for l in range(len(env_test.data)-1):\n",
        "        pi_out, next_history = model.pi(torch.tensor(state[-1], dtype=torch.float), history)\n",
        "        action = pi_out[0].tolist()\n",
        "        next_state, reward, done, info = env_test.step(action)\n",
        " \n",
        "        state = next_state[:]\n",
        "        history = next_history\n",
        "        \n",
        "        gen = info[0]; bid = info[1]; disp = info[2]; revenue = info[3]\n",
        "        bid_test[n_epi] += [bid]\n",
        "        mae_test[n_epi] += [abs(gen - bid)]\n",
        "        mbe_test[n_epi] += [abs(disp - bid)]\n",
        "        rev_test[n_epi] += [revenue]\n",
        "    \n",
        "    if (n_epi+1)%print_interval == 0:\n",
        "        MAE_train = round(100*np.mean(mae_train[n_epi]),2)\n",
        "        MAE_val   = round(100*np.mean(mae_val[n_epi]),2)\n",
        "        MAE_test  = round(100*np.mean(mae_test[n_epi]),2)\n",
        "        MBE_train = round(100*np.mean(mbe_train[n_epi]),2)\n",
        "        MBE_val   = round(100*np.mean(mbe_val[n_epi]),2)\n",
        "        MBE_test  = round(100*np.mean(mbe_test[n_epi]),2)\n",
        "        REV_train = round(max_price*RE_Capacity1*np.mean(rev_train[n_epi]),3)\n",
        "        REV_val   = round(max_price*RE_Capacity2*np.mean(rev_val[n_epi]),3)\n",
        "        REV_test  = round(max_price*RE_Capacity3*np.mean(rev_test[n_epi]),3)\n",
        " \n",
        "        print(\"episode: {}\".format(n_epi+1))\n",
        "        print(\"MAE_train: {}%\".format(MAE_train).ljust(25), end=\"\")\n",
        "        print(\"MAE_val: {}%\".format(MAE_val).ljust(25), end=\"\")\n",
        "        print(\"MAE_test: {}%\".format(MAE_test).ljust(25))\n",
        "        print(\"MBE_train: {}%\".format(MBE_train).ljust(25), end=\"\")\n",
        "        print(\"MBE_val: {}%\".format(MBE_val).ljust(25), end=\"\")\n",
        "        print(\"MBE_test: {}%\".format(MBE_test).ljust(25))\n",
        "        print(\"REV_train: ${}\".format(REV_train).ljust(25), end=\"\")\n",
        "        print(\"REV_val: ${}\".format(REV_val).ljust(25), end=\"\")\n",
        "        print(\"REV_test: ${}\".format(REV_test).ljust(25))\n",
        "        print(\"------------------------------------------------------------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode: 1\n",
            "MAE_train: 30.59%        MAE_val: 30.55%          MAE_test: 32.57%         \n",
            "MBE_train: 30.0%         MBE_val: 30.55%          MBE_test: 32.57%         \n",
            "REV_train: $-89.994      REV_val: $-116.355       REV_test: $-87.196       \n",
            "------------------------------------------------------------------------------------------\n",
            "episode: 2\n",
            "MAE_train: 21.95%        MAE_val: 13.48%          MAE_test: 15.03%         \n",
            "MBE_train: 19.57%        MBE_val: 12.76%          MBE_test: 14.21%         \n",
            "REV_train: $13.974       REV_val: $108.306        REV_test: $82.54         \n",
            "------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-3af2ad001824>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_epi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mtrain_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-28f2e26718b8>\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(model, batch, optimizer)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZmiVd55l8mI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4602ca16-7ad1-4d6a-d89e-7cae8699ae7d"
      },
      "source": [
        "# Environment\n",
        " \n",
        "select_num = np.argmin(np.mean(mbe_val[:-1],axis=1))\n",
        "select_test = np.array(bid_test[select_num][:])\n",
        "select_test_real = np.array(data_test[1:])\n",
        "select_test_price = np.array(price_test[1:])\n",
        " \n",
        "E = E_max/2\n",
        "mbe = []\n",
        "reward = []\n",
        "info = []\n",
        "for i in range(len(select_test)):\n",
        "    bid = select_test[i]\n",
        "    gen = select_test_real[i]\n",
        "    imb = select_test_price[i]\n",
        "    \n",
        "    soc = E/E_max\n",
        "    Voc = a0*np.exp(-a1*soc) + a2 + a3*soc - a4*soc**2 + a5*soc**3\n",
        "    Rs  = b0*np.exp(-b1*soc) + b2 + b3*soc - b4*soc**2 + b5*soc**3\n",
        "    Rts = c0*np.exp(-c1*soc) + c2\n",
        "    Rtl = d0*np.exp(-d1*soc) + d2\n",
        "    R   = Rs + Rts + Rtl\n",
        " \n",
        "    I_cmax = 1000000*E_max*(soc_max - soc)/N/(Voc*tdelta)\n",
        "    I_dmax = 1000000*E_max*(soc - soc_min)/N/(Voc*tdelta)\n",
        "    p_cmax = N*(Voc*I_cmax + I_cmax**2*R)\n",
        "    p_dmax = N*(Voc*I_dmax - I_dmax**2*R)\n",
        " \n",
        "    P_cmax = p_cmax/1000000; P_dmax = p_dmax/1000000\n",
        "    P_c = min(max(gen-bid, 0), P_max, P_cmax)\n",
        "    P_d = min(max(bid-gen, 0), P_max, P_dmax)\n",
        "    p_c = 1000000*P_c/N; p_d = 1000000*P_d/N\n",
        " \n",
        "    I_c = -(Voc - np.sqrt(Voc**2 + 4*R*p_c))/(2*R)\n",
        "    I_d = (Voc - np.sqrt(Voc**2 - 4*R*p_d))/(2*R)\n",
        "    if not np.isclose(p_c, 0):\n",
        "        eff_c = (Voc*I_c)/p_c\n",
        "        E = E + eff_c*P_c*tdelta\n",
        "        disp = gen - P_c\n",
        "        info += [[gen, round(bid,4), 'C', round(P_c,4), round(disp,4), round(eff_c,4), round(E,4)]]\n",
        "    elif not np.isclose(p_d, 0):\n",
        "        eff_d = p_d/(Voc*I_d)\n",
        "        E = E - (1/eff_d)*P_d*tdelta\n",
        "        disp = gen + P_d\n",
        "        info += [[gen, round(bid,4), 'D', round(P_d,4), round(disp,4), round(eff_d,4), round(E,4)]]\n",
        "    else:\n",
        "        disp = gen\n",
        "        info += [[gen, round(bid,4), 'N', 'N', round(disp,4), 'N', round(E,4)]]\n",
        "    \n",
        "    mbe += [abs(bid - disp)]\n",
        "    reward += [(imb*disp - imb*abs(bid-disp) - beta*(P_c+P_d))*tdelta]\n",
        " \n",
        "MAE_test = round(100*np.mean(np.abs(select_test_real - select_test)),2)\n",
        "MBE_test = round(100*np.mean(mbe),2)\n",
        "print(\"MAE_test: {}%\".format(MAE_test))\n",
        "print(\"MBE_test: {}%\".format(MBE_test))\n",
        "print(\"REV_test: ${}\".format(round(max_price*RE_Capacity3*np.mean(reward),3)))\n",
        " \n",
        "# pd.DataFrame(select_test).to_csv(RE+\"_Model3_DeepComp.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE_test: 15.03%\n",
            "MBE_test: 14.21%\n",
            "REV_test: $82.54\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}